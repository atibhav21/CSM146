\documentclass[11pt]{article}

\newcommand{\cnum}{CM146}
\newcommand{\ced}{Winter 2018}
\newcommand{\ctitle}[3]{\title{\vspace{-0.5in}\cnum, \ced\\Problem Set #1: #2}}

\newcommand{\solution}[1]{{{\color{blue}{\bf Solution:} {#1}}}}
\usepackage[usenames,dvipsnames,svgnames,table,hyperref]{xcolor}

\renewcommand*{\theenumi}{\alph{enumi}}
\renewcommand*\labelenumi{(\theenumi)}
\renewcommand*{\theenumii}{\roman{enumii}}
\renewcommand*\labelenumii{\theenumii.}


\begin{document}
\ctitle{xx}{yy}
\author{}
\date{}
\maketitle
\vspace{-0.75in}

\section{Problem 1}
\begin{enumerate}
\item \solution{} \newline
TODO

\item \solution{} \newline
We know from Conditional Probability that
$$
P(D_i, y_i) = P(D_i | y = 0) \cdot P (y = 0) + P(D_i | y = 1) \cdot P (y = 1)
$$
$$
P(y_i = 1) = \theta, P(y_i = 0) = 1 - \theta
$$
$$
P(D_i, y_i) = \left(\frac{n!}{a_i ! b_i ! c_i !} \alpha_1^{a_i} \beta_1^{b_i} \gamma_1^{c_i} \right) \cdot \theta
				+ \left(\frac{n!}{a_i ! b_i ! c_i !} \alpha_0^{a_i} \beta_0^{b_i} \gamma_0^{c_i} \right) \cdot (1 - \theta)
$$
$$
\log P(D_i, y_i) = 
$$
\end{enumerate}

\newpage
\section{Problem 2}
\begin{enumerate}
\item \solution{} \newline
The two unspecified transition probabilities are: 
\begin{itemize}
	\item $q_{22} = P(q_{t+1} = 2 | q_t = 2) = 1 - P(q_{t+1} = 1 | q_t = 2) = 1 - 1 = 0$
	\item $q_{21} = P(q_{t+1} = 2 | q_t = 1) = 1 - P(q_{t+1} = 1 | q_t = 1) = 1 - 1 = 0$
\end{itemize}
The two unspecified outcome probabilities are:
\begin{itemize}
	\item $e_1 (B) = P(O_t = B | q_t = 1) = 1 - P(O_t = A | q_t = 1) = 1 - 0.99 = 0.01$
	\item $e_2 (A) = P(O_t = A | q_t = 2) = 1 - P(O_t = B | q_t = 2) = 1 - 0.51 = 0.49$ 
\end{itemize}

\item \solution{} \newline
The probability that the first symbol is A can be calculated as follows:
$$
P(first = A) = P(O_1 = A | q_1 = 1) \cdot P(q_1 = 1) + P(O_1 = A | q_1 = 2) \cdot P(q_1 = 2)
$$
$$
P(first = A) = (0.99) (0.49) + (0.49) (0.51) = 0.735
$$
The probability that the first symbol is B can be calculated as follows:
$$
P(first = B) = P(O_1 = B | q_1 = 1) \cdot P(q_1 = 1) + P(O_1 = B | q_1 = 2) \cdot P(q_1 = 2)
$$
$$
P(first = B) = (0.01) (0.49) + (0.51) (0.51) = 0.265
$$
Hence, the more frequent output in the first position is A

\item \solution{} \newline
TODO
\end{enumerate}

\newpage
\section{Problem 3}
\begin{enumerate}
\item 
The problem with minimizing over $\mu, c, k$ is that we are extremely prone to overfitting. With this kind of
loss function, we will generate a model that achieves $J(\mu, c, k) = 0$ on a training set of size $n$, by 
using $n$ cluster centers, where each cluster has the label that is assigned to it in the training data. This
is an overfitted model and would perform terribly on test sets. \newline
For a training set S with labels y, \newline
$$\mu = S$$
$$c = y$$
$$k = |S|$$
\item 

\end{enumerate}
\end{document}
